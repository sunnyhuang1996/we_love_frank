The best method from 3.1 is Naive Bayes. And we choose first n tweeter in both class as training dataset


training size             500       1000     1500     2000      2500     3000     3500    4000      4500     5000      5500
59.25
training accuracy (%)    60.9      60.85    59.4667  60.125    60.64    59.6667  59.4571  59.25   59.4778    59.25    59.3545

testing accuracy (%)    55.9889   53.2033   55.7103  54.3175   52.3677  54.3175  54.039   55.1532  55.4318   54.8747  55.9889




========== Comments ====================
The training accuracy keeps stable, while the testing accuracy decreases at first and then increases as the number of training data increases.
I think when adding more training data at first, we introduce more noise due to mislabel, mis-tag, counting features problems. Thus, it increases bias of model and reduces accuracy. 
And then majority of new data is correct. With increasing size of training data, we can decrease the proportion of noise and reduce their negative effect. The model is closer to correct model and accuracy increases.  
